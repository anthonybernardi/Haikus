{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux-a_weUVGVF"
      },
      "source": [
        "# Train word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Odfuc2xyVCwP",
        "outputId": "b6174a20-5314-44c9-c688-e121d83de2b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-14 10:19:19.414343: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import Input\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CToj1UGdXPpR"
      },
      "source": [
        "Load and clean data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6yKDKcTrXx61"
      },
      "outputs": [],
      "source": [
        "HAIKU_BEGIN = \"<h>\"\n",
        "HAIKU_END = \"</h>\"\n",
        "LINE_BEGIN = \"<s>\"\n",
        "LINE_END = \"</s>\"\n",
        "\n",
        "NGRAM_SIZE = 6\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "\n",
        "EMBEDDING_SIZE = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo079uXXWBmR",
        "outputId": "dd5a182f-4d2f-42bc-9682-7f6fdfbb6981"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['<h>', '<s>', 'delicate', 'savage', '</s>', '<s>', \"you'll\", 'never', 'hold', 'the', 'cinder', '</s>', '<s>', 'but', 'still', 'you', 'will', 'burn', '</s>', '</h>'], ['<h>', '<s>', 'our', 'destination', '</s>', '<s>', 'the', 'skyline', 'of', 'this', 'city', '</s>', '<s>', 'shining', 'horizon', '</s>', '</h>'], ['<h>', '<s>', 'a', 'splash', 'and', 'a', 'cry', '</s>', '<s>', 'words', 'pulled', 'from', 'the', 'riverside', '</s>', '<s>', 'dried', 'in', 'the', 'hot', 'sun', '</s>', '</h>'], ['<h>', '<s>', 'hurt', 'but', 'poised', 'for', 'war', '</s>', '<s>', 'sturdy', 'in', 'crestfallen', 'slumps', '</s>', '<s>', 'warrior', 'spirit', '</s>', '</h>'], ['<h>', '<s>', 'steamy', 'mist', 'rising', '</s>', '<s>', 'rocks', 'receiving', 'downward', 'crash', '</s>', '<s>', 'as', 'the', 'jungle', 'weeps', '</s>', '</h>']]\n"
          ]
        }
      ],
      "source": [
        "# as per meeting w/ felix, training on whole haiku so it learns the structure\n",
        "# results = lists of tokenized haiku, with poem and line separator tokens:\n",
        "# [[<H>,<S>,stanza 1,</S>,<S>,stanza 2,</S>,<S>,stanza 3,</S>,</H>],...]\n",
        "\n",
        "haiku_loc = \"data/haiku_reddit.txt\"\n",
        "reddit_tokens = []\n",
        "with open(haiku_loc, 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "    tokens = []\n",
        "    # remove trailing spaces and end-of-poem $/n marker\n",
        "    stanzas = [s.strip(' $\\n') for s in line.split(\"/\")]\n",
        "    tokens.append(HAIKU_BEGIN)\n",
        "    for stanza in stanzas:\n",
        "      tokens.append(LINE_BEGIN)\n",
        "      # whitespace split rather than NLTK tokenize because I don't know if the\n",
        "      # syllable dictionary has entries for nonword NLTK tokens (eg 'll n't)\n",
        "      tokens.extend(stanza.split())\n",
        "      tokens.append(LINE_END)\n",
        "    tokens.append(HAIKU_END)\n",
        "    reddit_tokens.append(tokens)\n",
        "\n",
        "print(reddit_tokens[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvnN0vSOeXam"
      },
      "source": [
        "Train embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81GN5Ue8eZkR"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Trains a word2vec model on the given sentences. Returns the trained word embeddings as a KeyedVectors object.\n",
        "Function provided from HW4 starter code.\n",
        "\"\"\"\n",
        "def train_model(sentences, sg=1, window_size=5, vector_size=EMBEDDING_SIZE, min_count=1) :\n",
        "  model = Word2Vec(sentences=sentences, vector_size=vector_size, window=window_size, min_count=min_count, sg=sg)\n",
        "  return model.wv\n",
        "\n",
        "reddit_haiku_embs = train_model(reddit_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ds4TEM75hw5u",
        "outputId": "353ab9a7-3fde-47c8-833a-7871b6eaf584"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[3, 1, 1444, 3133, 2, 1, 918, 68, 334, 5, 7333, 2, 1, 28, 62, 11, 33, 555, 2, 4], [3, 1, 57, 3134, 2, 1, 5, 2068, 12, 26, 451, 2, 1, 796, 615, 2, 4], [3, 1, 7, 2069, 13, 7, 437, 2, 1, 81, 1568, 36, 5, 7334, 2, 1, 2304, 10, 5, 274, 65, 2, 4], [3, 1, 518, 28, 4983, 16, 506, 2, 1, 3853, 10, 7335, 7336, 2, 1, 2305, 438, 2, 4], [3, 1, 3854, 1114, 538, 2, 1, 1005, 3855, 3135, 1275, 2, 1, 41, 5, 2070, 1569, 2, 4]]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(reddit_tokens)\n",
        "encoded = tokenizer.texts_to_sequences(reddit_tokens)\n",
        "\n",
        "print(encoded[0:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtX8Q7jvlsbJ"
      },
      "outputs": [],
      "source": [
        "def generate_ngram_training_samples(encoded: list) -> list:\n",
        "    '''\n",
        "    Takes the encoded data (list of lists) and \n",
        "    generates the training samples out of it.\n",
        "    Parameters:\n",
        "    up to you, we've put in what we used\n",
        "    but you can add/remove as needed\n",
        "    return: \n",
        "    tuple of (training_x, training_y) in the format [[1, 2, 3], [2, 3, 2], ...] and [2, 4, ...]\n",
        "    '''\n",
        "    training_x = []\n",
        "    training_y = []\n",
        "\n",
        "    for sentence in encoded:\n",
        "      for i in range(len(sentence) - NGRAM_SIZE + 1):\n",
        "        training_x.append(sentence[i:i + NGRAM_SIZE - 1])\n",
        "        training_y.append(sentence[i + NGRAM_SIZE - 1])\n",
        "\n",
        "    return (training_x, training_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKODP3ZKlxU3",
        "outputId": "fff2349c-dd5f-44c6-c36b-733cf0934ea3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[3, 1, 1444, 3133, 2], [1, 1444, 3133, 2, 1], [1444, 3133, 2, 1, 918], [3133, 2, 1, 918, 68], [2, 1, 918, 68, 334]]\n",
            "[1, 918, 68, 334, 5]\n"
          ]
        }
      ],
      "source": [
        "training_x, training_y = generate_ngram_training_samples(encoded)\n",
        "\n",
        "print(training_x[0:5])\n",
        "print(training_y[0:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvYp32sTnqeu"
      },
      "outputs": [],
      "source": [
        "def create_word_to_embedding(embs: KeyedVectors) -> dict:\n",
        "    \"\"\"\n",
        "    Creates a mapping from each word in the embedding vocabulary to its embedding.\n",
        "    \"\"\"\n",
        "    word_to_embedding = {}\n",
        "    for word in embs.key_to_index.keys():\n",
        "      word_to_embedding[word] = embs[word]\n",
        "    return word_to_embedding\n",
        "\n",
        "def create_index_to_embedding(embs: KeyedVectors, tokenizer: Tokenizer) -> dict:\n",
        "  \"\"\"\n",
        "  Creates a mapping from the tokenizer index of each word in the embedding vocabulary to its embedding.\n",
        "  \"\"\"\n",
        "  index_to_embedding = {}\n",
        "  for word in embs.key_to_index.keys():\n",
        "    index = tokenizer.word_index[word]\n",
        "    index_to_embedding[index] = embs[word]\n",
        "  return index_to_embedding\n",
        "\n",
        "def get_word_to_index(word: str, tokenizer: Tokenizer):\n",
        "  return tokenizer.texts_to_sequences([[word]])[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ER9c6GYEpCLv"
      },
      "outputs": [],
      "source": [
        "word_to_embedding = create_word_to_embedding(reddit_haiku_embs)\n",
        "index_to_embedding = create_index_to_embedding(reddit_haiku_embs, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eULBI9fiuJKJ"
      },
      "outputs": [],
      "source": [
        "def data_generator(X: list, y: list, num_sequences_per_batch: int, i_to_emb: dict):\n",
        "    '''\n",
        "    Returns data generator to be used by feed_forward\n",
        "    https://wiki.python.org/moin/Generators\n",
        "    https://realpython.com/introduction-to-python-generators/\n",
        "    \n",
        "    Yields batches of embeddings and labels to go with them.\n",
        "    Use one hot vectors to encode the labels \n",
        "    (see the to_categorical function)\n",
        "\n",
        "    Requires a mapping to convert from tokenizer index to embedding vector.\n",
        "    \n",
        "    '''  \n",
        "    embs = []\n",
        "    labels = []\n",
        "    i = 0\n",
        "    while True:\n",
        "      i = i % len(X)\n",
        "\n",
        "      emb = [i_to_emb[n] for n in X[i]]  # [ [..200..], [..200..] ] list of lists, shape (n-1, embedding_size)\n",
        "      embs.append(emb)  # list of list of lists, shape (batch_size, n-1, emb_size)\n",
        "      # we want shape (batch_size, (n-1)*emb_size)\n",
        "\n",
        "      # create one-hot vector with the 1 at the location of the tokenizer index\n",
        "      # adding one to number fo classes to account for i_to_emb not containing 0\n",
        "      label = to_categorical(y[i], num_classes=len(i_to_emb)+1)\n",
        "      labels.append(label)\n",
        "      if len(embs) % num_sequences_per_batch == 0:\n",
        "        yield (np.reshape(embs, (num_sequences_per_batch, -1)), np.array(labels))\n",
        "        embs = []\n",
        "        labels = []\n",
        "      \n",
        "      i += 1\n",
        "      \n",
        "\n",
        "# sample = next(data_generator(training_x[:3], training_y[:3], 2, index_to_embedding))\n",
        "# print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sE8nhQE1uify"
      },
      "outputs": [],
      "source": [
        "train_generator = data_generator(training_x, training_y, BATCH_SIZE, index_to_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYDertZoVLNS"
      },
      "source": [
        "# Model 1: feedforward NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEs06_vsVUeb"
      },
      "outputs": [],
      "source": [
        "def build_feedforward_model(input_units, hidden_units, output_units):\n",
        "  model = Sequential()\n",
        "  model.add(Input(shape=(input_units,)))  # inputs will be vectors of this length, batch size not specified\n",
        "  model.add(Dense(hidden_units, activation=\"softmax\"))\n",
        "  model.add(Dense(output_units, activation=\"softmax\"))\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), \n",
        "                loss=keras.losses.CategoricalCrossentropy())\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyHEqmRFVSUt"
      },
      "source": [
        "# Model 2: RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGdyYdHkVU-g"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
