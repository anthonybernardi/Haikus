{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ux-a_weUVGVF"
   },
   "source": [
    "# Train word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Odfuc2xyVCwP",
    "outputId": "b6174a20-5314-44c9-c688-e121d83de2b5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, LSTM, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.losses import Reduction\n",
    "from keras import Input\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6yKDKcTrXx61"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "HAIKU_BEGIN = \"<h>\"\n",
    "HAIKU_END = \"</h>\"\n",
    "LINE_BEGIN = \"<s>\"\n",
    "LINE_END = \"</s>\"\n",
    "\n",
    "NGRAM_SIZE = 7\n",
    "EMBEDDING_SIZE = 200\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "INPUT_UNITS = (NGRAM_SIZE - 1) * EMBEDDING_SIZE\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "line_structure = {1 : 5,\n",
    "                 2 : 7,\n",
    "                 3 : 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125927\n"
     ]
    }
   ],
   "source": [
    "word_to_syllable = {}\n",
    "\n",
    "with open(\"data/phoneticDictionary.csv\", 'r', encoding='utf_8') as f:\n",
    "    f.readline()\n",
    "    for line in f.readlines():\n",
    "        cols = line.split(',')\n",
    "        word_to_syllable[cols[1].strip(\"\\\"\")] = int(cols[3])\n",
    "print(len(word_to_syllable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qo079uXXWBmR",
    "outputId": "dd5a182f-4d2f-42bc-9682-7f6fdfbb6981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<h>', '<h>', '<h>', '<h>', '<h>', '<h>', '<s>', 'delicate', 'savage', '</s>', '<s>', \"you'll\", 'never', 'hold', 'the', 'cinder', '</s>', '<s>', 'but', 'still', 'you', 'will', 'burn', '</s>', '</h>', '</h>', '</h>', '</h>', '</h>', '</h>'], ['<h>', '<h>', '<h>', '<h>', '<h>', '<h>', '<s>', 'our', 'destination', '</s>', '<s>', 'the', 'skyline', 'of', 'this', 'city', '</s>', '<s>', 'shining', 'horizon', '</s>', '</h>', '</h>', '</h>', '</h>', '</h>', '</h>'], ['<h>', '<h>', '<h>', '<h>', '<h>', '<h>', '<s>', 'a', 'splash', 'and', 'a', 'cry', '</s>', '<s>', 'words', 'pulled', 'from', 'the', 'riverside', '</s>', '<s>', 'dried', 'in', 'the', 'hot', 'sun', '</s>', '</h>', '</h>', '</h>', '</h>', '</h>', '</h>'], ['<h>', '<h>', '<h>', '<h>', '<h>', '<h>', '<s>', 'hurt', 'but', 'poised', 'for', 'war', '</s>', '<s>', 'sturdy', 'in', 'crestfallen', 'slumps', '</s>', '<s>', 'warrior', 'spirit', '</s>', '</h>', '</h>', '</h>', '</h>', '</h>', '</h>'], ['<h>', '<h>', '<h>', '<h>', '<h>', '<h>', '<s>', 'steamy', 'mist', 'rising', '</s>', '<s>', 'rocks', 'receiving', 'downward', 'crash', '</s>', '<s>', 'as', 'the', 'jungle', 'weeps', '</s>', '</h>', '</h>', '</h>', '</h>', '</h>', '</h>']]\n"
     ]
    }
   ],
   "source": [
    "# as per meeting w/ felix, training on whole haiku so it learns the structure\n",
    "# results = lists of tokenized haiku, with poem and line separator tokens:\n",
    "# [[<H>,<S>,stanza 1,</S>,<S>,stanza 2,</S>,<S>,stanza 3,</S>,</H>],...]\n",
    "\n",
    "haiku_loc = \"data/haiku_reddit.txt\"\n",
    "reddit_tokens = []\n",
    "with open(haiku_loc, 'r', encoding='utf-8') as f:\n",
    "  for line in f:\n",
    "    tokens = []\n",
    "    # remove trailing spaces and end-of-poem $/n marker\n",
    "    stanzas = [s.strip(' $\\n') for s in line.split(\"/\")]\n",
    "    tokens += [HAIKU_BEGIN] * (NGRAM_SIZE - 1)\n",
    "    \n",
    "    for stanza in stanzas:\n",
    "      tokens.append(LINE_BEGIN)\n",
    "      # whitespace split rather than NLTK tokenize because I don't know if the\n",
    "      # syllable dictionary has entries for nonword NLTK tokens (eg 'll n't)\n",
    "      tokens.extend(stanza.split())\n",
    "      tokens.append(LINE_END)\n",
    "      \n",
    "    tokens += [HAIKU_END] * (NGRAM_SIZE - 1)\n",
    "    reddit_tokens.append(tokens)\n",
    "\n",
    "print(reddit_tokens[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvnN0vSOeXam"
   },
   "source": [
    "Train embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "81GN5Ue8eZkR"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trains a word2vec model on the given sentences. Returns the trained word embeddings as a KeyedVectors object.\n",
    "Function provided from HW4 starter code.\n",
    "\"\"\"\n",
    "def train_model(sentences, sg=1, window_size=5, vector_size=EMBEDDING_SIZE, min_count=1) :\n",
    "  model = Word2Vec(sentences=sentences, vector_size=vector_size, window=window_size, min_count=min_count, sg=sg)\n",
    "  return model.wv\n",
    "\n",
    "reddit_haiku_embs = train_model(reddit_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ds4TEM75hw5u",
    "outputId": "353ab9a7-3fde-47c8-833a-7871b6eaf584"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 1, 1, 3, 1444, 3133, 4, 3, 918, 68, 334, 5, 7333, 4, 3, 28, 62, 11, 33, 555, 4, 2, 2, 2, 2, 2, 2], [1, 1, 1, 1, 1, 1, 3, 57, 3134, 4, 3, 5, 2068, 12, 26, 451, 4, 3, 796, 615, 4, 2, 2, 2, 2, 2, 2], [1, 1, 1, 1, 1, 1, 3, 7, 2069, 13, 7, 437, 4, 3, 81, 1568, 36, 5, 7334, 4, 3, 2304, 10, 5, 274, 65, 4, 2, 2, 2, 2, 2, 2], [1, 1, 1, 1, 1, 1, 3, 518, 28, 4983, 16, 506, 4, 3, 3853, 10, 7335, 7336, 4, 3, 2305, 438, 4, 2, 2, 2, 2, 2, 2], [1, 1, 1, 1, 1, 1, 3, 3854, 1114, 538, 4, 3, 1005, 3855, 3135, 1275, 4, 3, 41, 5, 2070, 1569, 4, 2, 2, 2, 2, 2, 2]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(reddit_tokens)\n",
    "encoded = tokenizer.texts_to_sequences(reddit_tokens)\n",
    "\n",
    "print(encoded[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UtX8Q7jvlsbJ"
   },
   "outputs": [],
   "source": [
    "def generate_ngram_training_samples(encoded: list) -> list:\n",
    "    '''\n",
    "    Takes the encoded data (list of lists) and \n",
    "    generates the training samples out of it.\n",
    "    Parameters:\n",
    "    up to you, we've put in what we used\n",
    "    but you can add/remove as needed\n",
    "    return: \n",
    "    tuple of (training_x, training_y) in the format [[1, 2, 3], [2, 3, 2], ...] and [2, 4, ...]\n",
    "    '''\n",
    "    training_x = []\n",
    "    training_y = []\n",
    "\n",
    "    for sentence in encoded:\n",
    "      for i in range(len(sentence) - NGRAM_SIZE + 1):\n",
    "        training_x.append(sentence[i:i + NGRAM_SIZE - 1])\n",
    "        training_y.append(sentence[i + NGRAM_SIZE - 1])\n",
    "\n",
    "    return (training_x, training_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKODP3ZKlxU3",
    "outputId": "fff2349c-dd5f-44c6-c36b-733cf0934ea3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 3], [1, 1, 1, 1, 3, 1444], [1, 1, 1, 3, 1444, 3133], [1, 1, 3, 1444, 3133, 4]]\n",
      "(276808, 6)\n",
      "[3, 1444, 3133, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "training_x, training_y = generate_ngram_training_samples(encoded)\n",
    "\n",
    "print(training_x[0:5])\n",
    "print(np.shape(training_x))\n",
    "print(training_y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DvYp32sTnqeu"
   },
   "outputs": [],
   "source": [
    "def create_word_to_embedding(embs: KeyedVectors) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a mapping from each word in the embedding vocabulary to its embedding.\n",
    "    \"\"\"\n",
    "    word_to_embedding = {}\n",
    "    for word in embs.key_to_index.keys():\n",
    "      word_to_embedding[word] = embs[word]\n",
    "    return word_to_embedding\n",
    "\n",
    "def create_index_to_embedding(embs: KeyedVectors, tokenizer: Tokenizer) -> dict:\n",
    "  \"\"\"\n",
    "  Creates a mapping from the tokenizer index of each word in the embedding vocabulary to its embedding.\n",
    "  \"\"\"\n",
    "  index_to_embedding = {}\n",
    "  for word in embs.key_to_index.keys():\n",
    "    index = tokenizer.word_index[word]\n",
    "    index_to_embedding[index] = embs[word]\n",
    "  return index_to_embedding\n",
    "\n",
    "def get_word_to_index(word: str, tokenizer: Tokenizer):\n",
    "  return tokenizer.texts_to_sequences([[word]])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ER9c6GYEpCLv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14269\n"
     ]
    }
   ],
   "source": [
    "word_to_embedding = create_word_to_embedding(reddit_haiku_embs)\n",
    "index_to_embedding = create_index_to_embedding(reddit_haiku_embs, tokenizer)\n",
    "print(len(index_to_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "eULBI9fiuJKJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsample = next(data_generator(rnn_training_x, rnn_training_y, 2, index_to_embedding))\\nsample = next(data_generator(training_x, training_y, 33, index_to_embedding))\\nprint(sample)\\nprint(np.shape(sample[0])) # batch_size, emb_size * n-1 -- (concatenated embeddings of n-1-word sample)\\nprint(np.shape(sample[1])) # batch_size, len(index_to_embedding) -- (a one-hot vector for each nth word result)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_generator(X: list, y: list, num_sequences_per_batch: int, i_to_emb: dict):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "\n",
    "    Requires a mapping to convert from tokenizer index to embedding vector.\n",
    "    \n",
    "    '''  \n",
    "    embs = []\n",
    "    labels = []\n",
    "    i = 0\n",
    "    while True:\n",
    "        i = i % len(X)\n",
    "        \n",
    "        emb = [i_to_emb[n] for n in X[i]]  # [ [..200..], [..200..] ] list of lists, shape (n-1, embedding_size)\n",
    "        embs.append(emb)  # list of list of lists, shape (batch_size, n-1, emb_size)\n",
    "        # we want shape (batch_size, (n-1)*emb_size)\n",
    "\n",
    "        # create one-hot vector with the 1 at the location of the tokenizer index\n",
    "        # adding 1 to length to account for vector indices starting from 1 instead of 0\n",
    "        label = to_categorical(y[i], num_classes=len(i_to_emb)+1)\n",
    "        labels.append(label)\n",
    "        if len(embs) % num_sequences_per_batch == 0:\n",
    "            yield (np.reshape(embs, (num_sequences_per_batch, -1)), np.array(labels))\n",
    "            embs = []\n",
    "            labels = []\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "sample = next(data_generator(rnn_training_x, rnn_training_y, 2, index_to_embedding))\n",
    "sample = next(data_generator(training_x, training_y, 33, index_to_embedding))\n",
    "print(sample)\n",
    "print(np.shape(sample[0])) # batch_size, emb_size * n-1 -- (concatenated embeddings of n-1-word sample)\n",
    "print(np.shape(sample[1])) # batch_size, len(index_to_embedding) -- (a one-hot vector for each nth word result)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "sE8nhQE1uify"
   },
   "outputs": [],
   "source": [
    "train_generator = data_generator(training_x, training_y, BATCH_SIZE, index_to_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYDertZoVLNS"
   },
   "source": [
    "# Model 1: feedforward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BEs06_vsVUeb"
   },
   "outputs": [],
   "source": [
    "def build_feed_forward_model(input_units, hidden_units, output_units):\n",
    "  model = Sequential()\n",
    "  \n",
    "  model.add(Input(shape=(input_units,)))  # inputs will be vectors of this length, batch size not specified\n",
    "  model.add(Dense(hidden_units, activation=\"softmax\"))\n",
    "  model.add(Dense(output_units, activation=\"softmax\"))\n",
    "  \n",
    "  model.compile(optimizer=Adam(learning_rate=0.01), loss=CategoricalCrossentropy())\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hGdyYdHkVU-g"
   },
   "outputs": [],
   "source": [
    "output_units = len(reddit_haiku_embs.key_to_index.keys()) + 1\n",
    "hidden_units = 1000 #round((INPUT_UNITS + output_units) / 2)\n",
    "\n",
    "feed_forward_model = build_feed_forward_model(INPUT_UNITS, hidden_units, output_units)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2162/2162 [==============================] - 761s 351ms/step - loss: 4.8018\n",
      "Epoch 2/3\n",
      "2162/2162 [==============================] - 744s 344ms/step - loss: 4.2822\n",
      "Epoch 3/3\n",
      "2162/2162 [==============================] - 786s 363ms/step - loss: 4.2627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fe25f70c70>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_forward_model.fit(x=train_generator, epochs=EPOCHS, steps_per_epoch=len(training_x) // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ffnn_model_trained\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ffnn_model_trained\\assets\n"
     ]
    }
   ],
   "source": [
    "feed_forward_model.save(\"ffnn_model_trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyHEqmRFVSUt"
   },
   "source": [
    "# Model 2: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm produces 1 label per timestep, so timestep = n-1-gram \n",
    "# timestep size = NGRAM_SIZE - 1 (4)\n",
    "# timestep consists of n-1 word embeddings, each with some features\n",
    "# features = EMBEDDING_SIZE (200)\n",
    "# then we can do this for a certain number of batches, lets say 128 still\n",
    "# the inputs to the LSTM need to have shape (batch_size, timesteps, features)\n",
    "# so for us that means shape (BATCH_SIZE, NGRAM_SIZE-1, EMBEDDING_SIZE)\n",
    "\n",
    "# upshot: have to make a new data generator, sicne the FFNN one squished all the embeddings together\n",
    "\n",
    "# timestep = ngram length is supported by several example articles: \n",
    "# http://ethen8181.github.io/machine-learning/keras/rnn_language_model_basic_keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 6, 200)\n",
      "(128, 14270)\n"
     ]
    }
   ],
   "source": [
    "def rnn_data_generator(X: list, y: list, batch_size: int, i_to_emb: dict):\n",
    "    '''\n",
    "    Produces a data generator for an RNN.\n",
    "    Output data is of shape (batch_size, len(X[0]), len(i_to_emb.values()[1])\n",
    "    i.e. (batch_size, ngram_size - 1, embedding_size)\n",
    "    Output labels are one-hot vectors of shape (batch_size, len(i_to_emb.keys())+1)\n",
    "    i.e. (batch_size, vocab_size) \n",
    "    '''  \n",
    "    embs = []\n",
    "    labels = []\n",
    "    i = 0\n",
    "    while True:\n",
    "        i = i % len(X)\n",
    "        \n",
    "        emb = [i_to_emb[n] for n in X[i]]  # [ [..200..], [..200..] ] list of lists, shape (n-1, embedding_size)\n",
    "        embs.append(emb)  # list of list of lists, shape (batch_size, n-1, emb_size)\n",
    "\n",
    "        # create one-hot vector with the 1 at the location of the tokenizer index\n",
    "        # adding 1 to length to account for vector indices starting from 1 instead of 0\n",
    "        label = to_categorical(y[i], num_classes=len(i_to_emb)+1)\n",
    "        labels.append(label)\n",
    "        if len(embs) % batch_size == 0:\n",
    "            #yield (np.array(embs), np.reshape(labels, (batch_size, len(i_to_emb)+1, 1)))\n",
    "            yield (np.array(embs), np.array(labels))\n",
    "            embs = []\n",
    "            labels = []\n",
    "\n",
    "        i += 1\n",
    "\n",
    "rnn_training_generator = rnn_data_generator(training_x, training_y, BATCH_SIZE, index_to_embedding)\n",
    "sample = next(rnn_training_generator)\n",
    "print(np.shape(sample[0]))\n",
    "print(np.shape(sample[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_model(timestep_size, input_units, hidden_units, output_units):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # input size needs to be a tuple of (timesteps, features), \n",
    "    # per https://towardsdatascience.com/a-practical-guide-to-rnn-and-lstm-in-keras-980f176271bc\n",
    "    model.add(Input(shape=(timestep_size, input_units)))  # (4, 200)\n",
    "    model.add(LSTM(hidden_units))\n",
    "    model.add(Dense(output_units, activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss=CategoricalCrossentropy())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_7 (LSTM)               (None, 500)               1402000   \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 14270)             7149270   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,551,270\n",
      "Trainable params: 8,551,270\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "hidden_units = 500 # a lot of the examples used 128\n",
    "output_units = len(reddit_haiku_embs.key_to_index.keys()) + 1\n",
    "\n",
    "rnn_model = build_rnn_model(NGRAM_SIZE - 1, EMBEDDING_SIZE, hidden_units, output_units)\n",
    "print(rnn_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = rnn_model(next(rnn_training_generator)[0])\n",
    "#print(y)  # output of model is (batch_size, vocab_size), i.e. a one-hot vector for each timestep\n",
    "#print(rnn_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2162/2162 [==============================] - 811s 372ms/step - loss: 3.8471\n",
      "Epoch 2/3\n",
      "2162/2162 [==============================] - 803s 371ms/step - loss: 3.4368\n",
      "Epoch 3/3\n",
      "2162/2162 [==============================] - 795s 368ms/step - loss: 3.2701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fe679f32b0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.fit(x=rnn_training_generator, epochs=EPOCHS, steps_per_epoch=len(training_x) // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_11_layer_call_fn, lstm_cell_11_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: rnn_model_large_trained\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: rnn_model_large_trained\\assets\n"
     ]
    }
   ],
   "source": [
    "#rnn_model.save(\"rnn_model_trained\")  # NGRAM_SIZE = 7, 128 hidden units\n",
    "rnn_model.save(\"rnn_model_large_trained\") # NGRAM_SIZE = 7, 500 hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an earlier attempt, treating an entire haiku as a sequence, padding them all to the same length, \n",
    "# using each ngram as a timestep, and using the concatenated 800-unit embeddings for each ngram"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MAX_TOKENS = max([len(x) for x in reddit_tokens])\n",
    "TIMESTEPS = MAX_TOKENS - NGRAM_SIZE + 1\n",
    "print(MAX_TOKENS)\n",
    "print(TIMESTEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAX_TOKENS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# make all haiku token sequences the same length/# of timesteps\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# by padding the beginning with placeholder tokens (beginning so no other tokens lead to the placeholder)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# doing this to the encoded version bc keras's built-in function deals with integers, not strings\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m rnn_encoded \u001b[38;5;241m=\u001b[39m pad_sequences(encoded, maxlen\u001b[38;5;241m=\u001b[39m\u001b[43mMAX_TOKENS\u001b[49m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#print(rnn_encoded[:5])\u001b[39;00m\n\u001b[0;32m      7\u001b[0m rnn_training_x, rnn_training_y \u001b[38;5;241m=\u001b[39m generate_ngram_training_samples(lstm_encoded)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MAX_TOKENS' is not defined"
     ]
    }
   ],
   "source": [
    "# make all haiku token sequences the same length/# of timesteps\n",
    "# by padding the beginning with placeholder tokens (beginning so no other tokens lead to the placeholder)\n",
    "# doing this to the encoded version bc keras's built-in function deals with integers, not strings\n",
    "rnn_encoded = pad_sequences(encoded, maxlen=MAX_TOKENS)\n",
    "#print(rnn_encoded[:5])\n",
    "\n",
    "rnn_training_x, rnn_training_y = generate_ngram_training_samples(lstm_encoded)\n",
    "print(rnn_training_x[11:16])\n",
    "print(np.shape(rnn_training_x))\n",
    "print(rnn_training_y[11:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_data_generator(X, y, batch_size, timesteps, i_to_emb):\n",
    "    \"\"\"\n",
    "    Generates data suitable for an RNN (in timestep groups).\n",
    "    Produces a tuple of training samples, shape=(batch_size, timesteps, INPUT_SIZE)\n",
    "    and their accompanying labels, shape=(batch_size, timesteps, VOCAB_SIZE)\n",
    "    \"\"\"\n",
    "    dg = data_generator(X, y, timesteps, i_to_emb)  # use ffnn data generator to get timestep blocks\n",
    "    i = 0\n",
    "    xs = []\n",
    "    ys = []\n",
    "    while True:\n",
    "        i = i % len(X)\n",
    "        next_x, next_y = next(dg)  # (TIMESTEPS, INPUT_SIZE), (TIMESTEPS, VOCAB_SIZE)\n",
    "        xs.append(next_x)\n",
    "        ys.append(next_y)\n",
    "        if len(xs) % batch_size == 0:\n",
    "            yield (np.array(xs), np.array(ys)) # ((BATCH_SIZE, TIMESTEPS, INPUT_SIZE), (BATCH_SIZE, TIMESTEPS, VOCAB_SIZE))\n",
    "            xs = []\n",
    "            ys = []\n",
    "\n",
    "        i += 1\n",
    "\n",
    "rnn_train_generator = rnn_data_generator(rnn_training_x, rnn_training_y, 1, TIMESTEPS, index_to_embedding)\n",
    "sample = next(rnn_train_generator)\n",
    "print(np.shape(sample[0]))\n",
    "print(sample[0])\n",
    "print(np.shape(sample[1]))\n",
    "print(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_model(input_units, hidden_units, output_units):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # input size needs to be a tuple of (timesteps, features), \n",
    "    # per https://towardsdatascience.com/a-practical-guide-to-rnn-and-lstm-in-keras-980f176271bc\n",
    "    model.add(Input(shape=(TIMESTEPS, input_units)))\n",
    "    model.add(LSTM(hidden_units)) #, input_shape=(TIMESTEPS, input_units)))\n",
    "    model.add(Dense(output_units, activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss=CategoricalCrossentropy(axis=2)) # trying axis 2 to see if that fixes the error\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_units = len(reddit_haiku_embs.key_to_index.keys()) + 1\n",
    "hidden_units = 128  # lots of examples used 128 lstm units so I'm going with that for now\n",
    "\n",
    "rnn_model = build_rnn_model(INPUT_UNITS, hidden_units, output_units)\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.fit(x=rnn_train_generator, epochs=1, steps_per_epoch=len(rnn_training_x) // (1 * TIMESTEPS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Haikus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved models\n",
    "#feed_forward_model = load_model(\"ffnn_model_trained\")\n",
    "rnn_model_1 = load_model(\"rnn_model_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data_generator(X: list, num_sequences_per_batch: int, i_to_emb: dict, is_rnn=False) -> list:\n",
    "    '''\n",
    "    Returns data generator to be used for prediction data\n",
    "    \n",
    "    Yields batches of embeddings to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "\n",
    "    Requires a mapping to convert from tokenizer index to embedding vector.\n",
    "    \n",
    "    '''  \n",
    "    embs = []\n",
    "    for i in range(len(X)):\n",
    "        emb = [i_to_emb[n] for n in X[i]]  # [ [..200..], [..200..] ] list of lists, shape (n-1, embedding_size)\n",
    "        embs.append(emb)  # list of list of lists, shape (batch_size, n-1, emb_size)\n",
    "        # we want shape (batch_size, (n-1)*emb_size)\n",
    "\n",
    "        # create one-hot vector with the 1 at the location of the tokenizer index\n",
    "        if len(embs) % num_sequences_per_batch == 0:\n",
    "            if is_rnn:\n",
    "                yield np.array(embs)\n",
    "            else:\n",
    "                yield np.reshape(embs, (num_sequences_per_batch, -1))\n",
    "            embs = []\n",
    "\n",
    "\n",
    "def generate_haiku(model: Sequential, \n",
    "                 tokenizer: Tokenizer, \n",
    "                 seed: list,\n",
    "                 i_to_emb: dict,\n",
    "                 n_words: int,\n",
    "                  is_rnn=False):\n",
    "    '''\n",
    "    Generate a haiku from the given model\n",
    "    \n",
    "    Parameters:\n",
    "        model: your neural network\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        seed: [w1, w2, w(n-1)]\n",
    "        n_words: generate a sentence of length n_words\n",
    "    Returns: string sentence\n",
    "    '''\n",
    "    sentence = seed\n",
    "    sentence_indices = tokenizer.texts_to_sequences([seed])[0]\n",
    "\n",
    "    # make the input list for the model.predict\n",
    "    # format is the n_grams so [[1, 2], [2, 3], [3, 4] ...]\n",
    "    predict_input = []\n",
    "    for i in range(len(sentence_indices) - NGRAM_SIZE + 2):\n",
    "        predict_input += [sentence_indices[i:i + NGRAM_SIZE]]\n",
    "    \n",
    "    \n",
    "    n_words_generated = 0\n",
    "    while n_words_generated < n_words:\n",
    "        #print(\"predict input: \", predict_input)\n",
    "        if is_rnn:\n",
    "            gen = predict_data_generator(predict_input, len(predict_input), i_to_emb, is_rnn=True)\n",
    "        else:\n",
    "            gen = predict_data_generator(predict_input, len(predict_input), i_to_emb)\n",
    "        probabilities = model.predict(x=gen, verbose=None)[0]\n",
    "\n",
    "        all_word_counts = [i for i in range(len(i_to_emb.keys()) + 1)]\n",
    "        sampled_index = np.random.choice(all_word_counts, p=probabilities)\n",
    "        new_word = tokenizer.sequences_to_texts([[sampled_index]])[0]\n",
    "      \n",
    "        sentence.append(new_word)\n",
    "        sentence_indices.append(sampled_index)\n",
    "        predict_input.append(sentence_indices[-(NGRAM_SIZE - 1):])\n",
    "        predict_input = predict_input[1:]\n",
    "      \n",
    "        if sentence[-1] == HAIKU_END:\n",
    "            break\n",
    "      \n",
    "        n_words_generated += 1\n",
    "    \n",
    "\n",
    "    return \" \".join(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syllables(sentence: list, syllable_dictionary: dict) -> int:\n",
    "    '''\n",
    "    Counts the number of syllables in the given sentence.\n",
    "    Meta-tokens like line begin/end do not affect syllable count.\n",
    "    '''\n",
    "    \n",
    "    count = 0\n",
    "    for word in sentence:\n",
    "        if word in [HAIKU_BEGIN, HAIKU_END, LINE_BEGIN, LINE_END]:\n",
    "            count += 0\n",
    "        else:\n",
    "            count += syllable_dictionary[word]\n",
    "    \n",
    "    return count\n",
    "\n",
    "def generate_haiku_greedy(model: Sequential, tokenizer: Tokenizer, seed=None: list, i_to_emb: dict, is_rnn=False):\n",
    "    '''\n",
    "    Generates a haiku from the model, ensuring a syllable fit using a greedy algorithm.\n",
    "    \n",
    "    Seed (optional) should be a list of tokens of length NGRAM_SIZE - 1 for the model to predict from.\n",
    "    If not specified, NGRAM_SIZE-1 haiku begin tokens will be used.\n",
    "    '''\n",
    "    if seed is None:\n",
    "        sentence = [HAIKU_BEGIN] * (NGRAM_SIZE - 1)\n",
    "    else:\n",
    "        sentence = seed\n",
    "    \n",
    "    sentence_indices = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    \n",
    "    current_line = 1\n",
    "    \n",
    "    haiku = []\n",
    "    while True:\n",
    "        sentence_embs = [i_to_emb[i] for i in sentence_indices]\n",
    "        current_syllables = get_syllables(sentence)\n",
    "        \n",
    "        probabilities = model.predict(x=seed_embs, verbose=None)[0]\n",
    "        \n",
    "        all_word_indices = [i for i in range(len(i_to_emb.keys()) + 2)]\n",
    "        sampled_index = np.random.choice(all_word_counts, p=probabilities)\n",
    "        new_word = tokenizer.sequences_to_texts([[sampled_index]])[0]\n",
    "        \n",
    "        if current_syllables + get_syllables([new_word]) == line_structure(current_line):\n",
    "            sentence.append(new_word)\n",
    "            sentence.append(LINE_END)\n",
    "        \n",
    "    \n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h> <h> <h> <h> <h> <h> <s> planes </s> <s> a half new of moonlight </s> <s> perhaps those pull sad </s> </h>\n",
      "<h> <h> <h> <h> <h> <h> <s> prick colors </s> <s> this become look for us </s> </h>\n"
     ]
    }
   ],
   "source": [
    "#haiku = generate_haiku(feed_forward_model, tokenizer, [HAIKU_BEGIN] * (NGRAM_SIZE - 1), index_to_embedding, 30)\n",
    "\n",
    "rnn_large_haiku = generate_haiku(rnn_model, tokenizer, [HAIKU_BEGIN] * (NGRAM_SIZE - 1), index_to_embedding, 30, is_rnn=True)\n",
    "print(rnn_large_haiku)\n",
    "rnn_haiku = generate_haiku(rnn_model_1, tokenizer, [HAIKU_BEGIN] * (NGRAM_SIZE - 1), index_to_embedding, 30, is_rnn=True)\n",
    "print(rnn_haiku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h> <h> <h> <h> <h> <h> </s> </s> </s> </s> <s> is i meet who the sleep the by the at while the haiku the paint the remains the very the sand the times the the\n",
      "<h> <h> <h> <h> <h> <h> </s> </s> </s> </s> <s> maybe guilty a page the hands the than the heart the little made the way the the steps the mine the lord the while made\n"
     ]
    }
   ],
   "source": [
    "print(rnn_2_haiku)\n",
    "print(rnn_haiku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
