{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ux-a_weUVGVF"
   },
   "source": [
    "# Train word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Odfuc2xyVCwP",
    "outputId": "b6174a20-5314-44c9-c688-e121d83de2b5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.losses import Reduction\n",
    "from keras import Input\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6yKDKcTrXx61"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "HAIKU_BEGIN = \"<h>\"\n",
    "HAIKU_END = \"</h>\"\n",
    "LINE_BEGIN = \"<s>\"\n",
    "LINE_END = \"</s>\"\n",
    "\n",
    "NGRAM_SIZE = 5\n",
    "EMBEDDING_SIZE = 200\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "INPUT_UNITS = (NGRAM_SIZE - 1) * EMBEDDING_SIZE\n",
    "\n",
    "EPOCHS = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qo079uXXWBmR",
    "outputId": "dd5a182f-4d2f-42bc-9682-7f6fdfbb6981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<h>', '<h>', '<h>', '<h>', '<s>', 'delicate', 'savage', '</s>', '<s>', \"you'll\", 'never', 'hold', 'the', 'cinder', '</s>', '<s>', 'but', 'still', 'you', 'will', 'burn', '</s>', '</h>', '</h>', '</h>', '</h>'], ['<h>', '<h>', '<h>', '<h>', '<s>', 'our', 'destination', '</s>', '<s>', 'the', 'skyline', 'of', 'this', 'city', '</s>', '<s>', 'shining', 'horizon', '</s>', '</h>', '</h>', '</h>', '</h>'], ['<h>', '<h>', '<h>', '<h>', '<s>', 'a', 'splash', 'and', 'a', 'cry', '</s>', '<s>', 'words', 'pulled', 'from', 'the', 'riverside', '</s>', '<s>', 'dried', 'in', 'the', 'hot', 'sun', '</s>', '</h>', '</h>', '</h>', '</h>'], ['<h>', '<h>', '<h>', '<h>', '<s>', 'hurt', 'but', 'poised', 'for', 'war', '</s>', '<s>', 'sturdy', 'in', 'crestfallen', 'slumps', '</s>', '<s>', 'warrior', 'spirit', '</s>', '</h>', '</h>', '</h>', '</h>'], ['<h>', '<h>', '<h>', '<h>', '<s>', 'steamy', 'mist', 'rising', '</s>', '<s>', 'rocks', 'receiving', 'downward', 'crash', '</s>', '<s>', 'as', 'the', 'jungle', 'weeps', '</s>', '</h>', '</h>', '</h>', '</h>']]\n"
     ]
    }
   ],
   "source": [
    "# as per meeting w/ felix, training on whole haiku so it learns the structure\n",
    "# results = lists of tokenized haiku, with poem and line separator tokens:\n",
    "# [[<H>,<S>,stanza 1,</S>,<S>,stanza 2,</S>,<S>,stanza 3,</S>,</H>],...]\n",
    "\n",
    "haiku_loc = \"data/haiku_reddit.txt\"\n",
    "reddit_tokens = []\n",
    "with open(haiku_loc, 'r', encoding='utf-8') as f:\n",
    "  for line in f:\n",
    "    tokens = []\n",
    "    # remove trailing spaces and end-of-poem $/n marker\n",
    "    stanzas = [s.strip(' $\\n') for s in line.split(\"/\")]\n",
    "    tokens += [HAIKU_BEGIN] * (NGRAM_SIZE - 1)\n",
    "    \n",
    "    for stanza in stanzas:\n",
    "      tokens.append(LINE_BEGIN)\n",
    "      # whitespace split rather than NLTK tokenize because I don't know if the\n",
    "      # syllable dictionary has entries for nonword NLTK tokens (eg 'll n't)\n",
    "      tokens.extend(stanza.split())\n",
    "      tokens.append(LINE_END)\n",
    "      \n",
    "    tokens += [HAIKU_END] * (NGRAM_SIZE - 1)\n",
    "    reddit_tokens.append(tokens)\n",
    "\n",
    "print(reddit_tokens[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvnN0vSOeXam"
   },
   "source": [
    "Train embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "81GN5Ue8eZkR"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trains a word2vec model on the given sentences. Returns the trained word embeddings as a KeyedVectors object.\n",
    "Function provided from HW4 starter code.\n",
    "\"\"\"\n",
    "def train_model(sentences, sg=1, window_size=5, vector_size=EMBEDDING_SIZE, min_count=1) :\n",
    "  model = Word2Vec(sentences=sentences, vector_size=vector_size, window=window_size, min_count=min_count, sg=sg)\n",
    "  return model.wv\n",
    "\n",
    "reddit_haiku_embs = train_model(reddit_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ds4TEM75hw5u",
    "outputId": "353ab9a7-3fde-47c8-833a-7871b6eaf584"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 3, 1444, 3133, 4, 3, 918, 68, 334, 5, 7333, 4, 3, 28, 62, 11, 33, 555, 4, 2, 2, 2, 2], [1, 1, 1, 1, 3, 57, 3134, 4, 3, 5, 2068, 12, 26, 451, 4, 3, 796, 615, 4, 2, 2, 2, 2], [1, 1, 1, 1, 3, 7, 2069, 13, 7, 437, 4, 3, 81, 1568, 36, 5, 7334, 4, 3, 2304, 10, 5, 274, 65, 4, 2, 2, 2, 2], [1, 1, 1, 1, 3, 518, 28, 4983, 16, 506, 4, 3, 3853, 10, 7335, 7336, 4, 3, 2305, 438, 4, 2, 2, 2, 2], [1, 1, 1, 1, 3, 3854, 1114, 538, 4, 3, 1005, 3855, 3135, 1275, 4, 3, 41, 5, 2070, 1569, 4, 2, 2, 2, 2]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(reddit_tokens)\n",
    "encoded = tokenizer.texts_to_sequences(reddit_tokens)\n",
    "\n",
    "print(encoded[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UtX8Q7jvlsbJ"
   },
   "outputs": [],
   "source": [
    "def generate_ngram_training_samples(encoded: list) -> list:\n",
    "    '''\n",
    "    Takes the encoded data (list of lists) and \n",
    "    generates the training samples out of it.\n",
    "    Parameters:\n",
    "    up to you, we've put in what we used\n",
    "    but you can add/remove as needed\n",
    "    return: \n",
    "    tuple of (training_x, training_y) in the format [[1, 2, 3], [2, 3, 2], ...] and [2, 4, ...]\n",
    "    '''\n",
    "    training_x = []\n",
    "    training_y = []\n",
    "\n",
    "    for sentence in encoded:\n",
    "      for i in range(len(sentence) - NGRAM_SIZE + 1):\n",
    "        training_x.append(sentence[i:i + NGRAM_SIZE - 1])\n",
    "        training_y.append(sentence[i + NGRAM_SIZE - 1])\n",
    "\n",
    "    return (training_x, training_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKODP3ZKlxU3",
    "outputId": "fff2349c-dd5f-44c6-c36b-733cf0934ea3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1], [1, 1, 1, 3], [1, 1, 3, 1444], [1, 3, 1444, 3133], [3, 1444, 3133, 4]]\n",
      "(254270, 4)\n",
      "[3, 1444, 3133, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "training_x, training_y = generate_ngram_training_samples(encoded)\n",
    "\n",
    "print(training_x[0:5])\n",
    "print(np.shape(training_x))\n",
    "print(training_y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DvYp32sTnqeu"
   },
   "outputs": [],
   "source": [
    "def create_word_to_embedding(embs: KeyedVectors) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a mapping from each word in the embedding vocabulary to its embedding.\n",
    "    \"\"\"\n",
    "    word_to_embedding = {}\n",
    "    for word in embs.key_to_index.keys():\n",
    "      word_to_embedding[word] = embs[word]\n",
    "    return word_to_embedding\n",
    "\n",
    "def create_index_to_embedding(embs: KeyedVectors, tokenizer: Tokenizer) -> dict:\n",
    "  \"\"\"\n",
    "  Creates a mapping from the tokenizer index of each word in the embedding vocabulary to its embedding.\n",
    "  \"\"\"\n",
    "  index_to_embedding = {}\n",
    "  for word in embs.key_to_index.keys():\n",
    "    index = tokenizer.word_index[word]\n",
    "    index_to_embedding[index] = embs[word]\n",
    "  return index_to_embedding\n",
    "\n",
    "def get_word_to_index(word: str, tokenizer: Tokenizer):\n",
    "  return tokenizer.texts_to_sequences([[word]])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "ER9c6GYEpCLv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14269\n"
     ]
    }
   ],
   "source": [
    "word_to_embedding = create_word_to_embedding(reddit_haiku_embs)\n",
    "index_to_embedding = create_index_to_embedding(reddit_haiku_embs, tokenizer)\n",
    "print(len(index_to_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "eULBI9fiuJKJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsample = next(data_generator(rnn_training_x, rnn_training_y, 2, index_to_embedding))\\nsample = next(data_generator(training_x, training_y, 33, index_to_embedding))\\nprint(sample)\\nprint(np.shape(sample[0])) # batch_size, emb_size * n-1 -- (concatenated embeddings of n-1-word sample)\\nprint(np.shape(sample[1])) # batch_size, len(index_to_embedding) -- (a one-hot vector for each nth word result)\\n'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_generator(X: list, y: list, num_sequences_per_batch: int, i_to_emb: dict):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "\n",
    "    Requires a mapping to convert from tokenizer index to embedding vector.\n",
    "    \n",
    "    '''  \n",
    "    embs = []\n",
    "    labels = []\n",
    "    i = 0\n",
    "    while True:\n",
    "        i = i % len(X)\n",
    "        \n",
    "        emb = [i_to_emb[n] for n in X[i]]  # [ [..200..], [..200..] ] list of lists, shape (n-1, embedding_size)\n",
    "        embs.append(emb)  # list of list of lists, shape (batch_size, n-1, emb_size)\n",
    "        # we want shape (batch_size, (n-1)*emb_size)\n",
    "\n",
    "        # create one-hot vector with the 1 at the location of the tokenizer index\n",
    "        # adding 1 to length to account for vector indices starting from 1 instead of 0\n",
    "        label = to_categorical(y[i], num_classes=len(i_to_emb)+1)\n",
    "        labels.append(label)\n",
    "        if len(embs) % num_sequences_per_batch == 0:\n",
    "            yield (np.reshape(embs, (num_sequences_per_batch, -1)), np.array(labels))\n",
    "            embs = []\n",
    "            labels = []\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "sample = next(data_generator(rnn_training_x, rnn_training_y, 2, index_to_embedding))\n",
    "sample = next(data_generator(training_x, training_y, 33, index_to_embedding))\n",
    "print(sample)\n",
    "print(np.shape(sample[0])) # batch_size, emb_size * n-1 -- (concatenated embeddings of n-1-word sample)\n",
    "print(np.shape(sample[1])) # batch_size, len(index_to_embedding) -- (a one-hot vector for each nth word result)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "sE8nhQE1uify"
   },
   "outputs": [],
   "source": [
    "train_generator = data_generator(training_x, training_y, BATCH_SIZE, index_to_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYDertZoVLNS"
   },
   "source": [
    "# Model 1: feedforward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BEs06_vsVUeb"
   },
   "outputs": [],
   "source": [
    "def build_feed_forward_model(input_units, hidden_units, output_units):\n",
    "  model = Sequential()\n",
    "  \n",
    "  model.add(Input(shape=(input_units,)))  # inputs will be vectors of this length, batch size not specified\n",
    "  model.add(Dense(hidden_units, activation=\"softmax\"))\n",
    "  model.add(Dense(output_units, activation=\"softmax\"))\n",
    "  \n",
    "  model.compile(optimizer=Adam(learning_rate=0.01), loss=CategoricalCrossentropy())\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "hGdyYdHkVU-g"
   },
   "outputs": [],
   "source": [
    "output_units = len(reddit_haiku_embs.key_to_index.keys()) + 1\n",
    "hidden_units = 1000 #round((INPUT_UNITS + output_units) / 2)\n",
    "\n",
    "feed_forward_model = build_feed_forward_model(INPUT_UNITS, hidden_units, output_units)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1986/1986 [==============================] - 652s 328ms/step - loss: 5.4882\n",
      "Epoch 2/3\n",
      "1986/1986 [==============================] - 633s 319ms/step - loss: 5.2078\n",
      "Epoch 3/3\n",
      "1986/1986 [==============================] - 647s 326ms/step - loss: 5.2055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28ee0d9c4c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_forward_model.fit(x=train_generator, epochs=EPOCHS, steps_per_epoch=len(training_x) // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyHEqmRFVSUt"
   },
   "source": [
    "# Model 2: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm produces 1 label per timestep, so timestep = n-1-gram \n",
    "# timestep size = NGRAM_SIZE - 1 (4)\n",
    "# timestep consists of n-1 word embeddings, each with some features\n",
    "# features = EMBEDDING_SIZE (200)\n",
    "# then we can do this for a certain number of batches, lets say 128 still\n",
    "# the inputs to the LSTM need to have shape (batch_size, timesteps, features)\n",
    "# so for us that means shape (BATCH_SIZE, NGRAM_SIZE-1, EMBEDDING_SIZE)\n",
    "\n",
    "# upshot: have to make a new data generator, sicne the FFNN one squished all the embeddings together\n",
    "\n",
    "# timestep = ngram length is supported by several example articles: \n",
    "# http://ethen8181.github.io/machine-learning/keras/rnn_language_model_basic_keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 4, 200)\n",
      "(128, 14270)\n"
     ]
    }
   ],
   "source": [
    "def rnn_data_generator(X: list, y: list, batch_size: int, i_to_emb: dict):\n",
    "    '''\n",
    "    Produces a data generator for an RNN.\n",
    "    Output data is of shape (batch_size, len(X[0]), len(i_to_emb.values()[1])\n",
    "    i.e. (batch_size, ngram_size - 1, embedding_size)\n",
    "    Output labels are one-hot vectors of shape (batch_size, len(i_to_emb.keys())+1)\n",
    "    i.e. (batch_size, vocab_size) \n",
    "    '''  \n",
    "    embs = []\n",
    "    labels = []\n",
    "    i = 0\n",
    "    while True:\n",
    "        i = i % len(X)\n",
    "        \n",
    "        emb = [i_to_emb[n] for n in X[i]]  # [ [..200..], [..200..] ] list of lists, shape (n-1, embedding_size)\n",
    "        embs.append(emb)  # list of list of lists, shape (batch_size, n-1, emb_size)\n",
    "\n",
    "        # create one-hot vector with the 1 at the location of the tokenizer index\n",
    "        # adding 1 to length to account for vector indices starting from 1 instead of 0\n",
    "        label = to_categorical(y[i], num_classes=len(i_to_emb)+1)\n",
    "        labels.append(label)\n",
    "        if len(embs) % batch_size == 0:\n",
    "            #yield (np.array(embs), np.reshape(labels, (batch_size, len(i_to_emb)+1, 1)))\n",
    "            yield (np.array(embs), np.array(labels))\n",
    "            embs = []\n",
    "            labels = []\n",
    "\n",
    "        i += 1\n",
    "\n",
    "rnn_training_generator = rnn_data_generator(training_x, training_y, BATCH_SIZE, index_to_embedding)\n",
    "sample = next(rnn_training_generator)\n",
    "print(np.shape(sample[0]))\n",
    "print(np.shape(sample[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_model(timestep_size, input_units, hidden_units, output_units):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # input size needs to be a tuple of (timesteps, features), \n",
    "    # per https://towardsdatascience.com/a-practical-guide-to-rnn-and-lstm-in-keras-980f176271bc\n",
    "    model.add(Input(shape=(timestep_size, input_units)))  # (4, 200)\n",
    "    model.add(LSTM(hidden_units))\n",
    "    model.add(Dense(output_units, activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss=CategoricalCrossentropy())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_31 (LSTM)              (None, 128)               168448    \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 14270)             1840830   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,009,278\n",
      "Trainable params: 2,009,278\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "hidden_units = 128 # that's what a lot of the examples seemed to be doing\n",
    "output_units = len(reddit_haiku_embs.key_to_index.keys()) + 1\n",
    "\n",
    "rnn_model = build_rnn_model(NGRAM_SIZE - 1, EMBEDDING_SIZE, hidden_units, output_units)\n",
    "print(rnn_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[6.9376372e-05 6.9513182e-05 7.0567563e-05 ... 7.0344518e-05\n",
      "  6.9256515e-05 7.1100949e-05]\n",
      " [6.9482165e-05 6.9473994e-05 7.0515896e-05 ... 7.0074646e-05\n",
      "  6.9247704e-05 7.1027112e-05]\n",
      " [6.9442343e-05 6.9467053e-05 7.0476824e-05 ... 6.9797890e-05\n",
      "  6.9174625e-05 7.0980292e-05]\n",
      " ...\n",
      " [6.9003581e-05 6.9507259e-05 7.0737929e-05 ... 7.0097405e-05\n",
      "  6.8907400e-05 7.1294329e-05]\n",
      " [6.9178226e-05 6.9462585e-05 7.0618386e-05 ... 6.9785754e-05\n",
      "  6.9039503e-05 7.1081769e-05]\n",
      " [6.9372334e-05 6.9475522e-05 7.0458678e-05 ... 6.9605412e-05\n",
      "  6.9066373e-05 7.0948896e-05]], shape=(128, 14270), dtype=float32)\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_30 (LSTM)              (None, 128)               168448    \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 14270)             1840830   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,009,278\n",
      "Trainable params: 2,009,278\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "y = rnn_model(next(rnn_training_generator)[0])\n",
    "print(y)  # output of model is (batch_size, vocab_size), i.e. a one-hot vector for each timestep\n",
    "print(rnn_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1986/1986 [==============================] - 242s 119ms/step - loss: 4.3523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28ee135ff70>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.fit(x=rnn_training_generator, epochs=1, steps_per_epoch=len(training_x) // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an earlier attempt, treating an entire haiku as a sequence, padding them all to the same length, \n",
    "# using each ngram as a timestep, and using the concatenated 800-unit embeddings for each ngram"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MAX_TOKENS = max([len(x) for x in reddit_tokens])\n",
    "TIMESTEPS = MAX_TOKENS - NGRAM_SIZE + 1\n",
    "print(MAX_TOKENS)\n",
    "print(TIMESTEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 1, 1, 1]), array([1, 1, 1, 3]), array([   1,    1,    3, 1444]), array([   1,    3, 1444, 3133]), array([   3, 1444, 3133,    4])]\n",
      "(371877, 4)\n",
      "[3, 1444, 3133, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "# make all haiku token sequences the same length/# of timesteps\n",
    "# by padding the beginning with placeholder tokens (beginning so no other tokens lead to the placeholder)\n",
    "# doing this to the encoded version bc keras's built-in function deals with integers, not strings\n",
    "rnn_encoded = pad_sequences(encoded, maxlen=MAX_TOKENS)\n",
    "#print(rnn_encoded[:5])\n",
    "\n",
    "rnn_training_x, rnn_training_y = generate_ngram_training_samples(lstm_encoded)\n",
    "print(rnn_training_x[11:16])\n",
    "print(np.shape(rnn_training_x))\n",
    "print(rnn_training_y[11:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 33, 800)\n",
      "[[[ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [-0.01699003  0.09968022 -0.18684933 ... -0.3113918   0.127307\n",
      "   -0.28496829]\n",
      "  [-0.06192206  0.05058902 -0.13142373 ... -0.3113918   0.127307\n",
      "   -0.28496829]\n",
      "  [-0.02317478  0.1506047  -0.15441597 ... -0.3113918   0.127307\n",
      "   -0.28496829]]]\n",
      "(1, 33, 14270)\n",
      "[[[1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 1. ... 0. 0. 0.]\n",
      "  [0. 0. 1. ... 0. 0. 0.]\n",
      "  [0. 0. 1. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "def rnn_data_generator(X, y, batch_size, timesteps, i_to_emb):\n",
    "    \"\"\"\n",
    "    Generates data suitable for an RNN (in timestep groups).\n",
    "    Produces a tuple of training samples, shape=(batch_size, timesteps, INPUT_SIZE)\n",
    "    and their accompanying labels, shape=(batch_size, timesteps, VOCAB_SIZE)\n",
    "    \"\"\"\n",
    "    dg = data_generator(X, y, timesteps, i_to_emb)  # use ffnn data generator to get timestep blocks\n",
    "    i = 0\n",
    "    xs = []\n",
    "    ys = []\n",
    "    while True:\n",
    "        i = i % len(X)\n",
    "        next_x, next_y = next(dg)  # (TIMESTEPS, INPUT_SIZE), (TIMESTEPS, VOCAB_SIZE)\n",
    "        xs.append(next_x)\n",
    "        ys.append(next_y)\n",
    "        if len(xs) % batch_size == 0:\n",
    "            yield (np.array(xs), np.array(ys)) # ((BATCH_SIZE, TIMESTEPS, INPUT_SIZE), (BATCH_SIZE, TIMESTEPS, VOCAB_SIZE))\n",
    "            xs = []\n",
    "            ys = []\n",
    "\n",
    "        i += 1\n",
    "\n",
    "rnn_train_generator = rnn_data_generator(rnn_training_x, rnn_training_y, 1, TIMESTEPS, index_to_embedding)\n",
    "sample = next(rnn_train_generator)\n",
    "print(np.shape(sample[0]))\n",
    "print(sample[0])\n",
    "print(np.shape(sample[1]))\n",
    "print(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_model(input_units, hidden_units, output_units):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # input size needs to be a tuple of (timesteps, features), \n",
    "    # per https://towardsdatascience.com/a-practical-guide-to-rnn-and-lstm-in-keras-980f176271bc\n",
    "    model.add(Input(shape=(TIMESTEPS, input_units)))\n",
    "    model.add(LSTM(hidden_units)) #, input_shape=(TIMESTEPS, input_units)))\n",
    "    model.add(Dense(output_units, activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss=CategoricalCrossentropy(axis=2)) # trying axis 2 to see if that fixes the error\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_18 (LSTM)              (None, 128)               475648    \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 14270)             1840830   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,316,478\n",
      "Trainable params: 2,316,478\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "output_units = len(reddit_haiku_embs.key_to_index.keys()) + 1\n",
    "hidden_units = 128  # lots of examples used 128 lstm units so I'm going with that for now\n",
    "\n",
    "rnn_model = build_rnn_model(INPUT_UNITS, hidden_units, output_units)\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'categorical_crossentropy/remove_squeezable_dimensions/Squeeze' defined at (most recent call last):\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\asyncio\\events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 531, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\acast\\AppData\\Local\\Temp\\ipykernel_15608\\2567760955.py\", line 1, in <module>\n      rnn_model.fit(x=rnn_train_generator, epochs=1, steps_per_epoch=len(rnn_training_x) // (1 * TIMESTEPS))\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\engine\\training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\engine\\training.py\", line 1109, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\losses.py\", line 261, in call\n      y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\utils\\losses_utils.py\", line 200, in squeeze_or_expand_dimensions\n      y_true, y_pred = remove_squeezable_dimensions(y_true, y_pred)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\utils\\losses_utils.py\", line 139, in remove_squeezable_dimensions\n      labels = tf.squeeze(labels, [-1])\nNode: 'categorical_crossentropy/remove_squeezable_dimensions/Squeeze'\nCan not squeeze dim[2], expected a dimension of 1, got 14270\n\t [[{{node categorical_crossentropy/remove_squeezable_dimensions/Squeeze}}]] [Op:__inference_train_function_36299]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[152], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrnn_train_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrnn_training_x\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTIMESTEPS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\haiku\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'categorical_crossentropy/remove_squeezable_dimensions/Squeeze' defined at (most recent call last):\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\asyncio\\events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 531, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\acast\\AppData\\Local\\Temp\\ipykernel_15608\\2567760955.py\", line 1, in <module>\n      rnn_model.fit(x=rnn_train_generator, epochs=1, steps_per_epoch=len(rnn_training_x) // (1 * TIMESTEPS))\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\engine\\training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\engine\\training.py\", line 1109, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\losses.py\", line 261, in call\n      y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\utils\\losses_utils.py\", line 200, in squeeze_or_expand_dimensions\n      y_true, y_pred = remove_squeezable_dimensions(y_true, y_pred)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\utils\\losses_utils.py\", line 139, in remove_squeezable_dimensions\n      labels = tf.squeeze(labels, [-1])\nNode: 'categorical_crossentropy/remove_squeezable_dimensions/Squeeze'\nCan not squeeze dim[2], expected a dimension of 1, got 14270\n\t [[{{node categorical_crossentropy/remove_squeezable_dimensions/Squeeze}}]] [Op:__inference_train_function_36299]"
     ]
    }
   ],
   "source": [
    "rnn_model.fit(x=rnn_train_generator, epochs=1, steps_per_epoch=len(rnn_training_x) // (1 * TIMESTEPS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Haikus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data_generator(X: list, num_sequences_per_batch: int, i_to_emb: dict) -> list:\n",
    "    '''\n",
    "    Returns data generator to be used for prediction data\n",
    "    \n",
    "    Yields batches of embeddings to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "\n",
    "    Requires a mapping to convert from tokenizer index to embedding vector.\n",
    "    \n",
    "    '''  \n",
    "    embs = []\n",
    "    for i in range(len(X)):\n",
    "      emb = [i_to_emb[n] for n in X[i]]  # [ [..200..], [..200..] ] list of lists, shape (n-1, embedding_size)\n",
    "      embs.append(emb)  # list of list of lists, shape (batch_size, n-1, emb_size)\n",
    "      # we want shape (batch_size, (n-1)*emb_size)\n",
    "\n",
    "      # create one-hot vector with the 1 at the location of the tokenizer index\n",
    "      if len(embs) % num_sequences_per_batch == 0:\n",
    "        yield np.reshape(embs, (num_sequences_per_batch, -1))\n",
    "        embs = []\n",
    "\n",
    "\n",
    "def generate_haiku(model: Sequential, \n",
    "                 tokenizer: Tokenizer, \n",
    "                 seed: list,\n",
    "                 i_to_emb: dict,\n",
    "                 n_words: int):\n",
    "    '''\n",
    "    Generate a haiku from the given model\n",
    "    \n",
    "    Parameters:\n",
    "        model: your neural network\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        seed: [w1, w2, w(n-1)]\n",
    "        n_words: generate a sentence of length n_words\n",
    "    Returns: string sentence\n",
    "    '''\n",
    "    sentence = seed\n",
    "    sentence_indices = tokenizer.texts_to_sequences([seed])[0]\n",
    "\n",
    "    # make the input list for the model.predict\n",
    "    # format is the n_grams so [[1, 2], [2, 3], [3, 4] ...]\n",
    "    predict_input = []\n",
    "    for i in range(len(sentence_indices) - NGRAM_SIZE + 2):\n",
    "      predict_input += [sentence_indices[i:i + NGRAM_SIZE]]\n",
    "    \n",
    "    n_words_generated = 0\n",
    "    while n_words_generated < n_words:\n",
    "      probabilities = model.predict(x=predict_data_generator(predict_input, len(predict_input), i_to_emb), verbose=None)[0]\n",
    "\n",
    "      all_word_counts = [i for i in range(1, len(i_to_emb.keys()) + 2)]\n",
    "      sampled_index = np.random.choice(all_word_counts, p=probabilities)\n",
    "      new_word = tokenizer.sequences_to_texts([[sampled_index]])[0]\n",
    "      \n",
    "      sentence.append(new_word)\n",
    "      sentence_indices.append(sampled_index)\n",
    "      predict_input.append(sentence_indices[-(NGRAM_SIZE - 1):])\n",
    "      \n",
    "      if sentence[-1] == HAIKU_END:\n",
    "        break\n",
    "      \n",
    "      n_words_generated += 1\n",
    "    \n",
    "\n",
    "    return \" \".join(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 235, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_33' (type Sequential).\n    \n    Input 0 of layer \"lstm_31\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, None)\n    \n    Call arguments received by layer 'sequential_33' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, None), dtype=float32)\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[223], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m haiku \u001b[38;5;241m=\u001b[39m generate_haiku(feed_forward_model, tokenizer, [HAIKU_BEGIN] \u001b[38;5;241m*\u001b[39m (NGRAM_SIZE \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m), index_to_embedding, \u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m rnn_haiku \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_haiku\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHAIKU_BEGIN\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mNGRAM_SIZE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_to_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 50\u001b[0m, in \u001b[0;36mgenerate_haiku\u001b[1;34m(model, tokenizer, seed, i_to_emb, n_words)\u001b[0m\n\u001b[0;32m     48\u001b[0m n_words_generated \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n_words_generated \u001b[38;5;241m<\u001b[39m n_words:\n\u001b[1;32m---> 50\u001b[0m   probabilities \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_data_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpredict_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi_to_emb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     52\u001b[0m   all_word_counts \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(i_to_emb\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m)]\n\u001b[0;32m     53\u001b[0m   sampled_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(all_word_counts, p\u001b[38;5;241m=\u001b[39mprobabilities)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileddvp3s8k.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\acast\\anaconda3\\envs\\haiku\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 235, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_33' (type Sequential).\n    \n    Input 0 of layer \"lstm_31\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, None)\n    \n    Call arguments received by layer 'sequential_33' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, None), dtype=float32)\n      • training=False\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "haiku = generate_haiku(feed_forward_model, tokenizer, [HAIKU_BEGIN] * (NGRAM_SIZE - 1), index_to_embedding, 30)\n",
    "rnn_haiku = generate_haiku(rnn_model, tokenizer, [HAIKU_BEGIN] * (NGRAM_SIZE - 1), index_to_embedding, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h> <h> <h> <h> know grey eyes <s> the see </s> the dawn the the <s> telling the </s> </s> wonder compost <s> </s> and is the </s> <s> the to burn wide never\n"
     ]
    }
   ],
   "source": [
    "print(haiku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
