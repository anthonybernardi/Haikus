{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux-a_weUVGVF"
      },
      "source": [
        "# Train word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Odfuc2xyVCwP",
        "outputId": "b6174a20-5314-44c9-c688-e121d83de2b5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import CategoricalCrossentropy\n",
        "from keras import Input\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "6yKDKcTrXx61"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "\n",
        "HAIKU_BEGIN = \"<h>\"\n",
        "HAIKU_END = \"</h>\"\n",
        "LINE_BEGIN = \"<s>\"\n",
        "LINE_END = \"</s>\"\n",
        "\n",
        "NGRAM_SIZE = 5\n",
        "EMBEDDING_SIZE = 200\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "INPUT_UNITS = (NGRAM_SIZE - 1) * EMBEDDING_SIZE\n",
        "\n",
        "EPOCHS = 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo079uXXWBmR",
        "outputId": "dd5a182f-4d2f-42bc-9682-7f6fdfbb6981"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['<h>', '<h>', '<h>', '<h>', '<s>', 'delicate', 'savage', '</s>', '<s>', \"you'll\", 'never', 'hold', 'the', 'cinder', '</s>', '<s>', 'but', 'still', 'you', 'will', 'burn', '</s>', '</h>', '</h>', '</h>', '</h>'], ['<h>', '<h>', '<h>', '<h>', '<s>', 'our', 'destination', '</s>', '<s>', 'the', 'skyline', 'of', 'this', 'city', '</s>', '<s>', 'shining', 'horizon', '</s>', '</h>', '</h>', '</h>', '</h>'], ['<h>', '<h>', '<h>', '<h>', '<s>', 'a', 'splash', 'and', 'a', 'cry', '</s>', '<s>', 'words', 'pulled', 'from', 'the', 'riverside', '</s>', '<s>', 'dried', 'in', 'the', 'hot', 'sun', '</s>', '</h>', '</h>', '</h>', '</h>'], ['<h>', '<h>', '<h>', '<h>', '<s>', 'hurt', 'but', 'poised', 'for', 'war', '</s>', '<s>', 'sturdy', 'in', 'crestfallen', 'slumps', '</s>', '<s>', 'warrior', 'spirit', '</s>', '</h>', '</h>', '</h>', '</h>'], ['<h>', '<h>', '<h>', '<h>', '<s>', 'steamy', 'mist', 'rising', '</s>', '<s>', 'rocks', 'receiving', 'downward', 'crash', '</s>', '<s>', 'as', 'the', 'jungle', 'weeps', '</s>', '</h>', '</h>', '</h>', '</h>']]\n"
          ]
        }
      ],
      "source": [
        "# as per meeting w/ felix, training on whole haiku so it learns the structure\n",
        "# results = lists of tokenized haiku, with poem and line separator tokens:\n",
        "# [[<H>,<S>,stanza 1,</S>,<S>,stanza 2,</S>,<S>,stanza 3,</S>,</H>],...]\n",
        "\n",
        "haiku_loc = \"data/haiku_reddit.txt\"\n",
        "reddit_tokens = []\n",
        "with open(haiku_loc, 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "    tokens = []\n",
        "    # remove trailing spaces and end-of-poem $/n marker\n",
        "    stanzas = [s.strip(' $\\n') for s in line.split(\"/\")]\n",
        "    tokens += [HAIKU_BEGIN] * (NGRAM_SIZE - 1)\n",
        "    \n",
        "    for stanza in stanzas:\n",
        "      tokens.append(LINE_BEGIN)\n",
        "      # whitespace split rather than NLTK tokenize because I don't know if the\n",
        "      # syllable dictionary has entries for nonword NLTK tokens (eg 'll n't)\n",
        "      tokens.extend(stanza.split())\n",
        "      tokens.append(LINE_END)\n",
        "      \n",
        "    tokens += [HAIKU_END] * (NGRAM_SIZE - 1)\n",
        "    reddit_tokens.append(tokens)\n",
        "\n",
        "print(reddit_tokens[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvnN0vSOeXam"
      },
      "source": [
        "Train embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "81GN5Ue8eZkR"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Trains a word2vec model on the given sentences. Returns the trained word embeddings as a KeyedVectors object.\n",
        "Function provided from HW4 starter code.\n",
        "\"\"\"\n",
        "def train_model(sentences, sg=1, window_size=5, vector_size=EMBEDDING_SIZE, min_count=1) :\n",
        "  model = Word2Vec(sentences=sentences, vector_size=vector_size, window=window_size, min_count=min_count, sg=sg)\n",
        "  return model.wv\n",
        "\n",
        "reddit_haiku_embs = train_model(reddit_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ds4TEM75hw5u",
        "outputId": "353ab9a7-3fde-47c8-833a-7871b6eaf584"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1, 1, 1, 1, 3, 1444, 3133, 4, 3, 918, 68, 334, 5, 7333, 4, 3, 28, 62, 11, 33, 555, 4, 2, 2, 2, 2], [1, 1, 1, 1, 3, 57, 3134, 4, 3, 5, 2068, 12, 26, 451, 4, 3, 796, 615, 4, 2, 2, 2, 2], [1, 1, 1, 1, 3, 7, 2069, 13, 7, 437, 4, 3, 81, 1568, 36, 5, 7334, 4, 3, 2304, 10, 5, 274, 65, 4, 2, 2, 2, 2], [1, 1, 1, 1, 3, 518, 28, 4983, 16, 506, 4, 3, 3853, 10, 7335, 7336, 4, 3, 2305, 438, 4, 2, 2, 2, 2], [1, 1, 1, 1, 3, 3854, 1114, 538, 4, 3, 1005, 3855, 3135, 1275, 4, 3, 41, 5, 2070, 1569, 4, 2, 2, 2, 2]]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(reddit_tokens)\n",
        "encoded = tokenizer.texts_to_sequences(reddit_tokens)\n",
        "\n",
        "print(encoded[0:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "UtX8Q7jvlsbJ"
      },
      "outputs": [],
      "source": [
        "def generate_ngram_training_samples(encoded: list) -> list:\n",
        "    '''\n",
        "    Takes the encoded data (list of lists) and \n",
        "    generates the training samples out of it.\n",
        "    Parameters:\n",
        "    up to you, we've put in what we used\n",
        "    but you can add/remove as needed\n",
        "    return: \n",
        "    tuple of (training_x, training_y) in the format [[1, 2, 3], [2, 3, 2], ...] and [2, 4, ...]\n",
        "    '''\n",
        "    training_x = []\n",
        "    training_y = []\n",
        "\n",
        "    for sentence in encoded:\n",
        "      for i in range(len(sentence) - NGRAM_SIZE + 1):\n",
        "        training_x.append(sentence[i:i + NGRAM_SIZE - 1])\n",
        "        training_y.append(sentence[i + NGRAM_SIZE - 1])\n",
        "\n",
        "    return (training_x, training_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKODP3ZKlxU3",
        "outputId": "fff2349c-dd5f-44c6-c36b-733cf0934ea3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1, 1, 1, 1], [1, 1, 1, 3], [1, 1, 3, 1444], [1, 3, 1444, 3133], [3, 1444, 3133, 4]]\n",
            "[3, 1444, 3133, 4, 3]\n"
          ]
        }
      ],
      "source": [
        "training_x, training_y = generate_ngram_training_samples(encoded)\n",
        "\n",
        "print(training_x[0:5])\n",
        "print(training_y[0:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "DvYp32sTnqeu"
      },
      "outputs": [],
      "source": [
        "def create_word_to_embedding(embs: KeyedVectors) -> dict:\n",
        "    \"\"\"\n",
        "    Creates a mapping from each word in the embedding vocabulary to its embedding.\n",
        "    \"\"\"\n",
        "    word_to_embedding = {}\n",
        "    for word in embs.key_to_index.keys():\n",
        "      word_to_embedding[word] = embs[word]\n",
        "    return word_to_embedding\n",
        "\n",
        "def create_index_to_embedding(embs: KeyedVectors, tokenizer: Tokenizer) -> dict:\n",
        "  \"\"\"\n",
        "  Creates a mapping from the tokenizer index of each word in the embedding vocabulary to its embedding.\n",
        "  \"\"\"\n",
        "  index_to_embedding = {}\n",
        "  for word in embs.key_to_index.keys():\n",
        "    index = tokenizer.word_index[word]\n",
        "    index_to_embedding[index] = embs[word]\n",
        "  return index_to_embedding\n",
        "\n",
        "def get_word_to_index(word: str, tokenizer: Tokenizer):\n",
        "  return tokenizer.texts_to_sequences([[word]])[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "ER9c6GYEpCLv"
      },
      "outputs": [],
      "source": [
        "word_to_embedding = create_word_to_embedding(reddit_haiku_embs)\n",
        "index_to_embedding = create_index_to_embedding(reddit_haiku_embs, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "eULBI9fiuJKJ"
      },
      "outputs": [],
      "source": [
        "def data_generator(X: list, y: list, num_sequences_per_batch: int, i_to_emb: dict):\n",
        "    '''\n",
        "    Returns data generator to be used by feed_forward\n",
        "    https://wiki.python.org/moin/Generators\n",
        "    https://realpython.com/introduction-to-python-generators/\n",
        "    \n",
        "    Yields batches of embeddings and labels to go with them.\n",
        "    Use one hot vectors to encode the labels \n",
        "    (see the to_categorical function)\n",
        "\n",
        "    Requires a mapping to convert from tokenizer index to embedding vector.\n",
        "    \n",
        "    '''  \n",
        "    embs = []\n",
        "    labels = []\n",
        "    i = 0\n",
        "    while True:\n",
        "      i = i % len(X)\n",
        "\n",
        "      emb = [i_to_emb[n] for n in X[i]]  # [ [..200..], [..200..] ] list of lists, shape (n-1, embedding_size)\n",
        "      embs.append(emb)  # list of list of lists, shape (batch_size, n-1, emb_size)\n",
        "      # we want shape (batch_size, (n-1)*emb_size)\n",
        "\n",
        "      # create one-hot vector with the 1 at the location of the tokenizer index\n",
        "      # adding one to number fo classes to account for i_to_emb not containing 0\n",
        "      label = to_categorical(y[i], num_classes=len(i_to_emb)+1)\n",
        "      labels.append(label)\n",
        "      if len(embs) % num_sequences_per_batch == 0:\n",
        "        yield (np.reshape(embs, (num_sequences_per_batch, -1)), np.array(labels))\n",
        "        embs = []\n",
        "        labels = []\n",
        "      \n",
        "      i += 1\n",
        "      \n",
        "\n",
        "# sample = next(data_generator(training_x[:3], training_y[:3], 2, index_to_embedding))\n",
        "# print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "sE8nhQE1uify"
      },
      "outputs": [],
      "source": [
        "train_generator = data_generator(training_x, training_y, BATCH_SIZE, index_to_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYDertZoVLNS"
      },
      "source": [
        "# Model 1: feedforward NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "BEs06_vsVUeb"
      },
      "outputs": [],
      "source": [
        "def build_feed_forward_model(input_units, hidden_units, output_units):\n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Input(shape=(input_units,)))  # inputs will be vectors of this length, batch size not specified\n",
        "  model.add(Dense(hidden_units, activation=\"softmax\"))\n",
        "  model.add(Dense(output_units, activation=\"softmax\"))\n",
        "  \n",
        "  model.compile(optimizer=Adam(learning_rate=0.01), loss=CategoricalCrossentropy())\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyHEqmRFVSUt"
      },
      "source": [
        "# Model 2: RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_rnn_model(input_units, hidden_units, output_units):\n",
        "    model = Sequential()\n",
        "    \n",
        "    print(input_units)\n",
        "    \n",
        "    # model.add(Input(shape=(input_units,))\n",
        "    model.add(LSTM(128, input_shape=(1, input_units)))\n",
        "    model.add(Dense(output_units, activation=\"softmax\"))\n",
        "    \n",
        "    model.compile(optimizer=Adam(learning_rate=0.01), loss=CategoricalCrossentropy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "hGdyYdHkVU-g"
      },
      "outputs": [],
      "source": [
        "output_units = len(reddit_haiku_embs.key_to_index.keys()) + 1\n",
        "hidden_units = 1000 #round((INPUT_UNITS + output_units) / 2)\n",
        "\n",
        "feed_forward_model = build_feed_forward_model(INPUT_UNITS, hidden_units, output_units)\n",
        "# rnn_model = build_rnn_model(INPUT_UNITS, hidden_units, output_units)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 23:49:20.390519: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1986/1986 [==============================] - 260s 131ms/step - loss: 5.4912\n",
            "Epoch 2/3\n",
            "1986/1986 [==============================] - 237s 119ms/step - loss: 5.2077\n",
            "Epoch 3/3\n",
            "1986/1986 [==============================] - 240s 121ms/step - loss: 5.2055\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x13e2a2cd0>"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feed_forward_model.fit(x=train_generator, epochs=EPOCHS, steps_per_epoch=len(training_x) // BATCH_SIZE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate Haikus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_data_generator(X: list, num_sequences_per_batch: int, i_to_emb: dict) -> list:\n",
        "    '''\n",
        "    Returns data generator to be used for prediction data\n",
        "    \n",
        "    Yields batches of embeddings to go with them.\n",
        "    Use one hot vectors to encode the labels \n",
        "    (see the to_categorical function)\n",
        "\n",
        "    Requires a mapping to convert from tokenizer index to embedding vector.\n",
        "    \n",
        "    '''  \n",
        "    embs = []\n",
        "    for i in range(len(X)):\n",
        "      emb = [i_to_emb[n] for n in X[i]]  # [ [..200..], [..200..] ] list of lists, shape (n-1, embedding_size)\n",
        "      embs.append(emb)  # list of list of lists, shape (batch_size, n-1, emb_size)\n",
        "      # we want shape (batch_size, (n-1)*emb_size)\n",
        "\n",
        "      # create one-hot vector with the 1 at the location of the tokenizer index\n",
        "      if len(embs) % num_sequences_per_batch == 0:\n",
        "        yield np.reshape(embs, (num_sequences_per_batch, -1))\n",
        "        embs = []\n",
        "\n",
        "\n",
        "def generate_haiku(model: Sequential, \n",
        "                 tokenizer: Tokenizer, \n",
        "                 seed: list,\n",
        "                 i_to_emb: dict,\n",
        "                 n_words: int):\n",
        "    '''\n",
        "    Generate a haiku from the given model\n",
        "    \n",
        "    Parameters:\n",
        "        model: your neural network\n",
        "        tokenizer: the keras preprocessing tokenizer\n",
        "        seed: [w1, w2, w(n-1)]\n",
        "        n_words: generate a sentence of length n_words\n",
        "    Returns: string sentence\n",
        "    '''\n",
        "    sentence = seed\n",
        "    sentence_indices = tokenizer.texts_to_sequences([seed])[0]\n",
        "\n",
        "    # make the input list for the model.predict\n",
        "    # format is the n_grams so [[1, 2], [2, 3], [3, 4] ...]\n",
        "    predict_input = []\n",
        "    for i in range(len(sentence_indices) - NGRAM_SIZE + 2):\n",
        "      predict_input += [sentence_indices[i:i + NGRAM_SIZE]]\n",
        "    \n",
        "    n_words_generated = 0\n",
        "    while n_words_generated < n_words:\n",
        "      probabilities = model.predict(x=predict_data_generator(predict_input, len(predict_input), i_to_emb), verbose=None)[0]\n",
        "\n",
        "      all_word_counts = [i for i in range(1, len(i_to_emb.keys()) + 2)]\n",
        "      sampled_index = np.random.choice(all_word_counts, p=probabilities)\n",
        "      new_word = tokenizer.sequences_to_texts([[sampled_index]])[0]\n",
        "      \n",
        "      sentence.append(new_word)\n",
        "      sentence_indices.append(sampled_index)\n",
        "      predict_input.append(sentence_indices[-(NGRAM_SIZE - 1):])\n",
        "      \n",
        "      if sentence[-1] == HAIKU_END:\n",
        "        break\n",
        "      \n",
        "      n_words_generated += 1\n",
        "    \n",
        "\n",
        "    return \" \".join(sentence)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-16 00:02:53.496762: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:53.577149: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:53.655096: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:53.740042: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:53.834268: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:53.912024: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:54.012111: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:54.098842: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:54.180387: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:54.261689: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:54.340659: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:54.415643: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:54.495049: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:54.576233: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:54.657095: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:54.736179: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:54.822589: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:54.891156: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:54.972051: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:55.060481: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:55.138237: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:55.221384: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:55.291488: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:55.371065: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:55.462745: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:55.561582: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:55.649974: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:55.735545: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:55.809256: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-16 00:02:55.884073: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        }
      ],
      "source": [
        "haiku = generate_haiku(feed_forward_model, tokenizer, [HAIKU_BEGIN] * (NGRAM_SIZE - 1), index_to_embedding, 30)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<h> <h> <h> <h> <s> your failed </s> head rush choose closed </s> the some i aside <s> </s> the <s> flickers he with the the tired fleeting the free rise <s> summer don't\n"
          ]
        }
      ],
      "source": [
        "print(haiku)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
